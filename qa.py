# -*- coding: utf-8 -*-
"""QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rsojmB2yh8Xv4kO6-cHyvjWaDIUMFJpI
"""

# Part 1: Data Preparation
import pandas as pd
import re
import unicodedata
from sklearn.model_selection import train_test_split

def prepare_data(input_csv_path='QA_Dataset_Hindi_Final.csv'):
    """
    Loads, cleans, filters, and splits the Hindi QA dataset.
    """
    print("Starting data preparation...")
    # Load the dataset
    df = pd.read_csv(input_csv_path)
    print(f"Initial dataset size: {df.shape}")

    # 1. Remove rows with English text
    def is_english(text):
        # Add logic here to check if text is English
        return False # Placeholder

    df = df[~df.apply(lambda row: any(is_english(cell) for cell in row), axis=1)]
    print(f"After removing English rows: {df.shape}")

    # 2. Clean and Normalize Text
    df.dropna(inplace=True)
    df = df[~df.apply(lambda row: any(str(cell).strip() == '' for cell in row), axis=1)]

    def clean_hindi_text(text):
        text = str(text).strip()
        text = re.sub(r'\s+', ' ', text)
        text = unicodedata.normalize("NFC", text)
        return text

    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].apply(clean_hindi_text)

    # 3. Drop duplicates
    df.drop_duplicates(inplace=True)
    print(f"After cleaning and dropping duplicates: {df.shape}")

    # 4. Format for Seq2Seq model
    df['input_text'] = "context: " + df['context'].astype(str)
    df['target_text'] = "question: " + df['question'].astype(str) + " answer: " + df['answer'].astype(str)
    formatted_df = df[['input_text', 'target_text']]

    # 5. Filter by length
    def is_valid_pair(row, min_c_len=30, max_c_len=512, min_t_len=10, max_t_len=128):
        return (min_c_len <= len(row['input_text']) <= max_c_len and
                min_t_len <= len(row['target_text']) <= max_t_len)
    filtered_df = formatted_df[formatted_df.apply(is_valid_pair, axis=1)].reset_index(drop=True)
    print(f"After filtering by length: {filtered_df.shape}")

    # 6. Split and save data
    train_df, test_df = train_test_split(filtered_df, test_size=0.2, random_state=42)
    val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)

    train_df.to_csv('train_data.csv', index=False)
    val_df.to_csv('val_data.csv', index=False)
    test_df.to_csv('test_data.csv', index=False)

    print("Data preparation complete. Files saved: train_data.csv, val_data.csv, test_data.csv")
    print(f"Train: {train_df.shape}, Validation: {val_df.shape}, Test: {test_df.shape}")

# ------------------------------------------------------------------------------------

# Part 2: Model Training
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import MBartTokenizer, MBartForConditionalGeneration
from torch.optim import AdamW
from tqdm import tqdm

class QADataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128):
        self.tokenizer = tokenizer
        self.inputs = dataframe["input_text"].tolist()
        self.targets = dataframe["target_text"].tolist()
        self.max_input_len = max_input_len
        self.max_target_len = max_target_len

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input_text = self.inputs[idx]
        target_text = self.targets[idx]

        input_encoding = self.tokenizer(input_text, max_length=self.max_input_len, padding="max_length", truncation=True, return_tensors="pt")
        target_encoding = self.tokenizer(target_text, max_length=self.max_target_len, padding="max_length", truncation=True, return_tensors="pt")

        labels = target_encoding["input_ids"].flatten()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_encoding["input_ids"].flatten(),
            "attention_mask": input_encoding["attention_mask"].flatten(),
            "labels": labels,
        }

def train_model():
    """
    Loads pre-trained mBART model and fine-tunes it on the prepared dataset.
    """
    print("\nStarting model training...")
    # Setup
    MODEL_NAME = "facebook/mbart-large-50-many-to-many-mmt"
    tokenizer = MBartTokenizer.from_pretrained(MODEL_NAME, src_lang="hi_IN", tgt_lang="hi_IN")
    model = MBartForConditionalGeneration.from_pretrained(MODEL_NAME)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Using device: {device}")

    # Load data
    train_df = pd.read_csv('train_data.csv').dropna()
    val_df = pd.read_csv('val_data.csv').dropna()

    train_dataset = QADataset(train_df, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

    # Optimizer
    optimizer = AdamW(model.parameters(), lr=3e-5)

    # Training Loop
    EPOCHS = 5
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        loop = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}")
        for batch in loop:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(train_loader)
        print(f"\nEpoch {epoch + 1} Average Training Loss: {avg_loss:.4f}")

    # Save model
    model.save_pretrained("./mbart_hindi_qa_model")
    tokenizer.save_pretrained("./mbart_hindi_qa_model")
    print("Training complete. Model saved to './mbart_hindi_qa_model'.")

# ------------------------------------------------------------------------------------

# Part 3: Gradio Application
import gradio as gr

def launch_app():
    """
    Loads the fine-tuned model and launches a Gradio web interface.
    """
    print("\nLaunching Gradio application...")

    # Load model and tokenizer
    model_dir = "./mbart_hindi_qa_model"
    tokenizer = MBartTokenizer.from_pretrained(model_dir)
    model = MBartForConditionalGeneration.from_pretrained(model_dir)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    def generate_qa_from_context(context):
        if not context.strip():
            return "<span style='color:red'><b>‡§™‡•ç‡§∞‡§∂‡•ç‡§®:</b> ---</span><br><span style='color:green'><b>‡§â‡§§‡•ç‡§§‡§∞:</b> ---</span>"

        input_text = f"context: {context}"
        inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True, padding="max_length").to(device)

        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=128,
            num_beams=10,              # Increase beams for better quality
            num_return_sequences=3,    # Generate 3 outputs
            early_stopping=True,
            do_sample=True,            # Sampling for diversity
            top_k=50,
            top_p=0.95
        )

        results = []
        for output in outputs:
            decoded_output = tokenizer.decode(output, skip_special_tokens=True)
            try:
                question_part = re.search(r"question:(.*?)answer:", decoded_output, re.DOTALL)
                answer_part = re.search(r"answer:(.*)", decoded_output, re.DOTALL)

                question = question_part.group(1).strip() if question_part else "Could not parse question."
                answer = answer_part.group(1).strip() if answer_part else "Could not parse answer."

            except Exception:
                question = "Error parsing output."
                answer = decoded_output

            results.append(f"<span style='color:red'><b>‡§™‡•ç‡§∞‡§∂‡•ç‡§®:</b> {question}</span><br><span style='color:green'><b>‡§â‡§§‡•ç‡§§‡§∞:</b> {answer}</span>")

        return "<hr>".join(results)

    # Gradio Interface
    iface = gr.Interface(
        fn=generate_qa_from_context,
        inputs=gr.Textbox(label="‡§™‡•à‡§∞‡§æ‡§ó‡•ç‡§∞‡§æ‡§´ ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç (Context)", lines=10, placeholder="‡§Ø‡§π‡§æ‡§Å ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Ö‡§®‡•Å‡§ö‡•ç‡§õ‡•á‡§¶ ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç..."),
        outputs=gr.HTML(label="‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§î‡§∞ ‡§â‡§§‡•ç‡§§‡§∞"),
        title="üáÆüá≥ Hindi Question-Answer Generator",
        description="‡§è‡§ï ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Ö‡§®‡•Å‡§ö‡•ç‡§õ‡•á‡§¶ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç, ‡§î‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§â‡§∏‡§∏‡•á ‡§è‡§ï ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§î‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡•á‡§ó‡§æ‡•§",
        examples=[
            ["‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ 1857 ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡§æ‡§Ç‡§§‡§ø ‡§∏‡•á ‡§Æ‡§æ‡§®‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§∞‡§æ‡§®‡•Ä ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡•Ä‡§¨‡§æ‡§à ‡§î‡§∞ ‡§§‡§æ‡§§‡•ç‡§Ø‡§æ ‡§ü‡•ã‡§™‡•á ‡§ú‡•à‡§∏‡•á ‡§®‡•á‡§§‡§æ‡§ì‡§Ç ‡§®‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ ‡§®‡§ø‡§≠‡§æ‡§à‡•§"],
            ["‡§™‡§∞‡•ç‡§Ø‡§æ‡§µ‡§∞‡§£ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§æ ‡§Æ‡•Ç‡§≤ ‡§Ü‡§ß‡§æ‡§∞ ‡§π‡•à‡•§ ‡§™‡•á‡§°‡§º-‡§™‡•å‡§ß‡•á, ‡§®‡§¶‡§ø‡§Ø‡§æ‡§Å, ‡§î‡§∞ ‡§µ‡§æ‡§Ø‡•Å ‡§∏‡§≠‡•Ä ‡§á‡§∏‡§ï‡•á ‡§π‡§ø‡§∏‡•ç‡§∏‡•á ‡§π‡•à‡§Ç‡•§ ‡§Æ‡§æ‡§®‡§µ ‡§ó‡§§‡§ø‡§µ‡§ø‡§ß‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§Ü‡§ú ‡§™‡§∞‡•ç‡§Ø‡§æ‡§µ‡§∞‡§£ ‡§™‡•ç‡§∞‡§¶‡•Ç‡§∑‡§£ ‡§§‡•á‡§ú‡•Ä ‡§∏‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡•§"]
        ]
    )
    iface.launch(share=True)

# Main execution block
if __name__ == '__main__':
    # Step 1: Prepare the data
    prepare_data('QA_Dataset_Hindi_Final.csv')

    # Step 2: Train the model (requires a GPU)
    train_model()

    # Step 3: Launch the application
    launch_app()